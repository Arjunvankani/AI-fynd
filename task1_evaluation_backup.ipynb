{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Yelp Review Rating Prediction - Task 1 Evaluation\n",
        "\n",
        "This notebook implements and evaluates 4 different prompting approaches for predicting Yelp review ratings (1-5 stars).\n",
        "\n",
        "## Approaches:\n",
        "1. **Zero-Shot**: Direct classification without examples\n",
        "2. **Few-Shot**: Classification with example reviews\n",
        "3. **Chain-of-Thought (CoT)**: Step-by-step reasoning\n",
        "4. **Hybrid (Few-Shot + CoT)**: Combines examples with reasoning\n",
        "\n",
        "## Evaluation Metrics:\n",
        "- Accuracy (Exact Match)\n",
        "- Mean Absolute Error (MAE)\n",
        "- JSON Validity Rate\n",
        "- Off-by-One Accuracy (within Â±1 star)\n",
        "- Reliability (consistency across runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ransformers (C:\\Users\\Arjun vankani\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ransformers (C:\\Users\\Arjun vankani\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ransformers (C:\\Users\\Arjun vankani\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
            "[notice] To update, run: C:\\Python313\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (including Google Generative AI)\n",
        "%pip install --quiet pandas numpy matplotlib seaborn scikit-learn requests openpyxl google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing google-generativeai...\n",
            "âœ“ google-generativeai installed successfully\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, confusion_matrix\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install google-generativeai if not already installed\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    print(\"âœ“ google-generativeai already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing google-generativeai...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"google-generativeai\"])\n",
        "    import google.generativeai as genai\n",
        "    print(\"âœ“ google-generativeai installed successfully\")\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Note: google.generativeai is already imported in cell 2 above\n",
        "\n",
        "GEMINI_API_KEY = \"AIzaSyAapHUDCKsUCRQg13mKHaZa8E01O_OEnCw\"  # Replace with your Gemini API key\n",
        "# Get your API key from: https://aistudio.google.com/app/apikey\n",
        "\n",
        "# Model Selection\n",
        "# Options:\n",
        "# - \"gemini-pro\" (Free tier, good for testing)\n",
        "# - \"gemini-1.5-pro\" (Better performance, may have usage limits)\n",
        "# - \"gemini-1.5-flash\" (Faster, free tier)\n",
        "MODEL_NAME = \"gemini-2.5-flash\"  # Change this\n",
        "\n",
        "# Configure Gemini API\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Dataset path (update to your dataset location)\n",
        "DATASET_PATH = \"yelp.csv\"  # Update this path (use yelp.csv from your folder)\n",
        "\n",
        "# Evaluation settings\n",
        "SAMPLE_SIZE = 200  # Number of reviews to evaluate\n",
        "TEMPERATURE = 0.0  # Deterministic outputs for consistency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def call_llm_api(prompt: str, max_retries: int = 3) -> str:\n",
        "    \"\"\"Call Gemini API with retry logic\"\"\"\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "    \n",
        "    generation_config = {\n",
        "        \"temperature\": TEMPERATURE,\n",
        "        \"max_output_tokens\": 500,\n",
        "    }\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=generation_config\n",
        "            )\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "            else:\n",
        "                return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "def extract_json(response: str) -> Dict:\n",
        "    \"\"\"Extract JSON from LLM response\"\"\"\n",
        "    try:\n",
        "        # Try direct JSON parsing\n",
        "        return json.loads(response)\n",
        "    except json.JSONDecodeError:\n",
        "        try:\n",
        "            # Try to find JSON block\n",
        "            start = response.find('{')\n",
        "            end = response.rfind('}') + 1\n",
        "            if start != -1 and end != 0:\n",
        "                return json.loads(response[start:end])\n",
        "        except:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "\n",
        "def load_dataset(path: str, sample_size: int = None) -> pd.DataFrame:\n",
        "    \"\"\"Load and sample Yelp reviews dataset\"\"\"\n",
        "    print(f\"Loading dataset from {path}...\")\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        print(f\"Loaded {len(df)} reviews\")\n",
        "        \n",
        "        # Sample if specified\n",
        "        if sample_size and sample_size < len(df):\n",
        "            df = df.sample(n=sample_size, random_state=42)\n",
        "            print(f\"Sampled {len(df)} reviews for evaluation\")\n",
        "        \n",
        "        # Ensure required columns exist\n",
        "        if 'text' not in df.columns or 'stars' not in df.columns:\n",
        "            raise ValueError(\"Dataset must have 'text' and 'stars' columns\")\n",
        "        \n",
        "        # Filter out invalid ratings\n",
        "        df = df[df['stars'].isin([1, 2, 3, 4, 5])]\n",
        "        \n",
        "        return df.reset_index(drop=True)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {path} not found. Please download the dataset from:\")\n",
        "        print(\"https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset\")\n",
        "        return pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 1: Zero-Shot Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def zero_shot_prompt(review_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Approach 1: Zero-Shot Classification\n",
        "    \n",
        "    Pros: Fast, simple, no examples needed\n",
        "    Cons: May struggle with ambiguous reviews\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Analyze the following Yelp review and predict the star rating (1-5 stars).\n",
        "\n",
        "Review: \"{review_text}\"\n",
        "\n",
        "Return a JSON object with the following structure:\n",
        "{{\n",
        "    \"predicted_stars\": <integer between 1 and 5>,\n",
        "    \"explanation\": \"<brief explanation for the rating>\"\n",
        "}}\n",
        "\n",
        "Consider the overall sentiment, specific compliments or complaints, and the reviewer's satisfaction level.\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def predict_zero_shot(review_text: str) -> Dict:\n",
        "    \"\"\"Predict rating using zero-shot approach\"\"\"\n",
        "    prompt = zero_shot_prompt(review_text)\n",
        "    response = call_llm_api(prompt)\n",
        "    result = extract_json(response)\n",
        "    \n",
        "    if result and 'predicted_stars' in result:\n",
        "        return result\n",
        "    return {\"predicted_stars\": None, \"explanation\": \"JSON parsing failed\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 2: Few-Shot Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def few_shot_prompt(review_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Approach 2: Few-Shot Classification\n",
        "    \n",
        "    Pros: Better accuracy, learns from examples\n",
        "    Cons: Longer prompts, more tokens\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an expert Yelp review rating classifier. Analyze reviews and predict star ratings (1-5).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Review 1: \"Terrible experience! Food was inedible, service was rude, and the place was dirty. Worst restaurant ever. Never coming back!\"\n",
        "Rating: {{ \"predicted_stars\": 1, \"explanation\": \"Extremely negative review with multiple severe complaints\" }}\n",
        "\n",
        "Review 2: \"Disappointing meal. The food was okay but nothing special. Service was slow and the waiter seemed uninterested. Expected better for the price.\"\n",
        "Rating: {{ \"predicted_stars\": 2, \"explanation\": \"Below average experience with multiple negatives\" }}\n",
        "\n",
        "Review 3: \"It was fine. Nothing great but nothing terrible either. Food was average, service was okay. Decent place for a quick meal.\"\n",
        "Rating: {{ \"predicted_stars\": 3, \"explanation\": \"Neutral review indicating average experience\" }}\n",
        "\n",
        "Review 4: \"Great food and friendly staff! Really enjoyed the meal. The atmosphere was nice and prices were reasonable. Would recommend to friends.\"\n",
        "Rating: {{ \"predicted_stars\": 4, \"explanation\": \"Positive review with multiple compliments and recommendation\" }}\n",
        "\n",
        "Review 5: \"Absolutely amazing! Best restaurant I've ever been to. Everything was perfect - the food, service, ambiance. Can't wait to come back!\"\n",
        "Rating: {{ \"predicted_stars\": 5, \"explanation\": \"Extremely positive review with highest praise\" }}\n",
        "\n",
        "Now analyze this review:\n",
        "Review: \"{review_text}\"\n",
        "\n",
        "Return a JSON object:\n",
        "{{\n",
        "    \"predicted_stars\": <integer 1-5>,\n",
        "    \"explanation\": \"<brief explanation>\"\n",
        "}}\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def predict_few_shot(review_text: str) -> Dict:\n",
        "    \"\"\"Predict rating using few-shot approach\"\"\"\n",
        "    prompt = few_shot_prompt(review_text)\n",
        "    response = call_llm_api(prompt)\n",
        "    result = extract_json(response)\n",
        "    \n",
        "    if result and 'predicted_stars' in result:\n",
        "        return result\n",
        "    return {\"predicted_stars\": None, \"explanation\": \"JSON parsing failed\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 3: Chain-of-Thought (CoT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chain_of_thought_prompt(review_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Approach 3: Chain-of-Thought Reasoning\n",
        "    \n",
        "    Pros: Better reasoning, handles complex cases\n",
        "    Cons: Longer responses, more tokens\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Analyze the following Yelp review step-by-step and predict the star rating.\n",
        "\n",
        "Review: \"{review_text}\"\n",
        "\n",
        "Think through your analysis:\n",
        "\n",
        "Step 1: Identify the overall sentiment (very negative, negative, neutral, positive, very positive)\n",
        "Step 2: Note specific complaints or compliments mentioned\n",
        "Step 3: Consider the intensity of the language used\n",
        "Step 4: Determine if the reviewer would recommend this place\n",
        "Step 5: Based on the above steps, assign a star rating (1-5)\n",
        "\n",
        "Return a JSON object:\n",
        "{{\n",
        "    \"predicted_stars\": <integer 1-5>,\n",
        "    \"explanation\": \"<explanation based on your reasoning steps>\"\n",
        "}}\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def predict_cot(review_text: str) -> Dict:\n",
        "    \"\"\"Predict rating using chain-of-thought approach\"\"\"\n",
        "    prompt = chain_of_thought_prompt(review_text)\n",
        "    response = call_llm_api(prompt)\n",
        "    result = extract_json(response)\n",
        "    \n",
        "    if result and 'predicted_stars' in result:\n",
        "        return result\n",
        "    return {\"predicted_stars\": None, \"explanation\": \"JSON parsing failed\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 4: Hybrid (Few-Shot + CoT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hybrid_prompt(review_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Approach 4: Hybrid (Few-Shot + Chain-of-Thought)\n",
        "    \n",
        "    Pros: Combines benefits of both approaches, best accuracy\n",
        "    Cons: Longest prompts, most tokens\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an expert Yelp review rating classifier. Analyze reviews systematically.\n",
        "\n",
        "Rating Guidelines:\n",
        "- 1 star: Terrible experience, multiple severe complaints, would not recommend\n",
        "- 2 stars: Disappointing, below expectations, significant issues\n",
        "- 3 stars: Average, acceptable but nothing special, neutral experience\n",
        "- 4 stars: Good experience, positive overall, would recommend\n",
        "- 5 stars: Excellent, exceptional experience, highest praise\n",
        "\n",
        "Example Analysis:\n",
        "Review: \"The food was decent but service was terrible. Waited 45 minutes for appetizers. Won't be back.\"\n",
        "Reasoning: Negative sentiment overall, specific complaint about service wait time, indicates dissatisfaction â†’ 2 stars\n",
        "\n",
        "Now analyze this review step-by-step:\n",
        "\n",
        "Review: \"{review_text}\"\n",
        "\n",
        "Step 1: Overall sentiment?\n",
        "Step 2: Key positive/negative points?\n",
        "Step 3: Intensity of feelings?\n",
        "Step 4: Recommendation likelihood?\n",
        "Step 5: Final rating (1-5)?\n",
        "\n",
        "Return a JSON object:\n",
        "{{\n",
        "    \"predicted_stars\": <integer 1-5>,\n",
        "    \"explanation\": \"<your reasoning and final assessment>\"\n",
        "}}\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def predict_hybrid(review_text: str) -> Dict:\n",
        "    \"\"\"Predict rating using hybrid approach\"\"\"\n",
        "    prompt = hybrid_prompt(review_text)\n",
        "    response = call_llm_api(prompt)\n",
        "    result = extract_json(response)\n",
        "    \n",
        "    if result and 'predicted_stars' in result:\n",
        "        return result\n",
        "    return {\"predicted_stars\": None, \"explanation\": \"JSON parsing failed\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_approach(\n",
        "    df: pd.DataFrame, \n",
        "    predict_func, \n",
        "    approach_name: str\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate a prompting approach on the dataset\n",
        "    \n",
        "    Returns dictionary with metrics:\n",
        "    - accuracy: Exact match rate\n",
        "    - mae: Mean Absolute Error\n",
        "    - json_validity: Percentage of valid JSON responses\n",
        "    - off_by_one: Accuracy within Â±1 star\n",
        "    - predictions: List of all predictions\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating: {approach_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    valid_json_count = 0\n",
        "    total_time = 0\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        review_text = str(row['text'])\n",
        "        actual_stars = int(row['stars'])\n",
        "        \n",
        "        # Get prediction\n",
        "        start_time = time.time()\n",
        "        result = predict_func(review_text)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        total_time += elapsed_time\n",
        "        \n",
        "        predicted_stars = result.get('predicted_stars')\n",
        "        \n",
        "        # Check JSON validity\n",
        "        if predicted_stars is not None and 1 <= predicted_stars <= 5:\n",
        "            valid_json_count += 1\n",
        "            predictions.append(predicted_stars)\n",
        "        else:\n",
        "            # Use neutral rating for invalid predictions\n",
        "            predictions.append(3)\n",
        "        \n",
        "        actuals.append(actual_stars)\n",
        "        \n",
        "        # Progress update\n",
        "        if (idx + 1) % 20 == 0:\n",
        "            print(f\"Processed {idx + 1}/{len(df)} reviews...\")\n",
        "        \n",
        "        # Rate limiting (adjust based on API limits)\n",
        "        time.sleep(0.1)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    predictions = np.array(predictions)\n",
        "    actuals = np.array(actuals)\n",
        "    \n",
        "    accuracy = accuracy_score(actuals, predictions)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    json_validity = valid_json_count / len(df)\n",
        "    \n",
        "    # Off-by-one accuracy\n",
        "    off_by_one = np.mean(np.abs(actuals - predictions) <= 1)\n",
        "    \n",
        "    # Average time per prediction\n",
        "    avg_time = total_time / len(df)\n",
        "    \n",
        "    results = {\n",
        "        'approach': approach_name,\n",
        "        'accuracy': accuracy,\n",
        "        'mae': mae,\n",
        "        'json_validity': json_validity,\n",
        "        'off_by_one_accuracy': off_by_one,\n",
        "        'avg_time_per_prediction': avg_time,\n",
        "        'predictions': predictions,\n",
        "        'actuals': actuals\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nResults for {approach_name}:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"  MAE: {mae:.4f}\")\n",
        "    print(f\"  JSON Validity: {json_validity:.4f} ({json_validity*100:.2f}%)\")\n",
        "    print(f\"  Off-by-One Accuracy: {off_by_one:.4f} ({off_by_one*100:.2f}%)\")\n",
        "    print(f\"  Avg Time/Prediction: {avg_time:.2f}s\")\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Dataset loaded successfully!\n",
            "   Reviews: 10000\n",
            "   Rating distribution:\n",
            "stars\n",
            "1     749\n",
            "2     927\n",
            "3    1461\n",
            "4    3526\n",
            "5    3337\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load and sample dataset\n",
        "df = pd.read_csv(\"yelp.csv\")\n",
        "\n",
        "if len(df) == 0:\n",
        "    print(\"\\nâš ï¸  Please download the dataset and update DATASET_PATH\")\n",
        "    print(\"Dataset: https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset\")\n",
        "else:\n",
        "    print(f\"\\nâœ… Dataset loaded successfully!\")\n",
        "    print(f\"   Reviews: {len(df)}\")\n",
        "    print(f\"   Rating distribution:\")\n",
        "    print(df['stars'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sampled 200 reviews for evaluation\n",
            "Sampled star distribution:\n",
            "stars\n",
            "1    40\n",
            "2    40\n",
            "3    40\n",
            "4    40\n",
            "5    40\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Sample 200 reviews for evaluation (balanced across star ratings)\n",
        "# This ensures we test on all rating levels\n",
        "sample_size = 200\n",
        "sample_per_star = sample_size // 5\n",
        "\n",
        "sampled_dfs = []\n",
        "for star in range(1, 6):\n",
        "    star_df = df[df['stars'] == star].sample(n=min(sample_per_star, len(df[df['stars'] == star])), random_state=42)\n",
        "    sampled_dfs.append(star_df)\n",
        "\n",
        "sample_df = pd.concat(sampled_dfs, ignore_index=True)\n",
        "sample_df = sample_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
        "\n",
        "print(f\"\\nSampled {len(sample_df)} reviews for evaluation\")\n",
        "print(f\"Sampled star distribution:\\n{sample_df['stars'].value_counts().sort_index()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Evaluating: Zero-Shot\n",
            "============================================================\n",
            "Processed 20/200 reviews...\n",
            "Processed 40/200 reviews...\n",
            "Processed 60/200 reviews...\n",
            "Processed 80/200 reviews...\n",
            "Processed 100/200 reviews...\n",
            "Processed 120/200 reviews...\n",
            "Processed 140/200 reviews...\n",
            "Processed 160/200 reviews...\n",
            "Processed 180/200 reviews...\n",
            "Processed 200/200 reviews...\n",
            "\n",
            "Results for Zero-Shot:\n",
            "  Accuracy: 0.2000 (20.00%)\n",
            "  MAE: 1.2000\n",
            "  JSON Validity: 0.0000 (0.00%)\n",
            "  Off-by-One Accuracy: 0.6000 (60.00%)\n",
            "  Avg Time/Prediction: 4.15s\n",
            "\n",
            "âœ… Saved results to CSV\n",
            "\n",
            "============================================================\n",
            "Evaluating: Few-Shot\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mcall_llm_api\u001b[39m\u001b[34m(prompt, max_retries)\u001b[39m\n\u001b[32m     23\u001b[39m response = requests.post(\n\u001b[32m     24\u001b[39m     OPENROUTER_URL, \n\u001b[32m     25\u001b[39m     headers=headers, \n\u001b[32m     26\u001b[39m     json=data, \n\u001b[32m     27\u001b[39m     timeout=\u001b[32m30\u001b[39m\n\u001b[32m     28\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()[\u001b[33m'\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arjun vankani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Evaluate each approach\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m predict_func, approach_name \u001b[38;5;129;01min\u001b[39;00m approaches:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     results = \u001b[43mevaluate_approach\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproach_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     all_results.append(results)\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Save individual results\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mevaluate_approach\u001b[39m\u001b[34m(df, predict_func, approach_name)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Get prediction\u001b[39;00m\n\u001b[32m     30\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m result = \u001b[43mpredict_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m elapsed_time = time.time() - start_time\n\u001b[32m     33\u001b[39m total_time += elapsed_time\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mpredict_few_shot\u001b[39m\u001b[34m(review_text)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Predict rating using few-shot approach\"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m prompt = few_shot_prompt(review_text)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m response = \u001b[43mcall_llm_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m result = extract_json(response)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mpredicted_stars\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mcall_llm_api\u001b[39m\u001b[34m(prompt, max_retries)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries - \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         time.sleep(\u001b[32m2\u001b[39m ** attempt)  \u001b[38;5;66;03m# Exponential backoff\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "if len(sample_df) > 0:\n",
        "    # Define approaches\n",
        "    approaches = [\n",
        "        (predict_zero_shot, \"Zero-Shot\"),\n",
        "        (predict_few_shot, \"Few-Shot\"),\n",
        "        (predict_cot, \"Chain-of-Thought\"),\n",
        "        (predict_hybrid, \"Hybrid (Few-Shot + CoT)\")\n",
        "    ]\n",
        "    \n",
        "    # Store all results\n",
        "    all_results = []\n",
        "    \n",
        "    # Evaluate each approach\n",
        "    for predict_func, approach_name in approaches:\n",
        "        results = evaluate_approach(sample_df, predict_func, approach_name)\n",
        "        all_results.append(results)\n",
        "        \n",
        "        # Save individual results\n",
        "        results_df = pd.DataFrame({\n",
        "            'actual_stars': results['actuals'],\n",
        "            'predicted_stars': results['predictions'],\n",
        "            'review_text': sample_df['text'].values\n",
        "        })\n",
        "        filename = f\"results_{approach_name.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')}.csv\"\n",
        "        results_df.to_csv(filename, index=False)\n",
        "        \n",
        "        print(f\"\\nâœ… Saved results to CSV\")\n",
        "        \n",
        "        # Brief pause between approaches\n",
        "        time.sleep(2)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ALL EVALUATIONS COMPLETE!\")\n",
        "    print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison Table and Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(df) > 0 and 'all_results' in locals():\n",
        "    # Create comparison table\n",
        "    comparison_data = []\n",
        "    for result in all_results:\n",
        "        comparison_data.append({\n",
        "            'Approach': result['approach'],\n",
        "            'Accuracy': f\"{result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\",\n",
        "            'MAE': f\"{result['mae']:.4f}\",\n",
        "            'JSON Validity': f\"{result['json_validity']:.4f} ({result['json_validity']*100:.2f}%)\",\n",
        "            'Off-by-One': f\"{result['off_by_one_accuracy']:.4f} ({result['off_by_one_accuracy']*100:.2f}%)\",\n",
        "            'Avg Time (s)': f\"{result['avg_time_per_prediction']:.2f}\"\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON TABLE\")\n",
        "    print(\"=\"*80)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Save comparison table\n",
        "    comparison_df.to_csv(\"comparison_table.csv\", index=False)\n",
        "    print(\"\\nâœ… Comparison table saved to comparison_table.csv\")\n",
        "    \n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # 1. Accuracy comparison\n",
        "    approaches_list = [r['approach'] for r in all_results]\n",
        "    accuracies = [r['accuracy'] for r in all_results]\n",
        "    \n",
        "    axes[0, 0].bar(approaches_list, accuracies, color='steelblue')\n",
        "    axes[0, 0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].set_ylim([0, 1])\n",
        "    axes[0, 0].tick_params(axis='x', rotation=15)\n",
        "    for i, v in enumerate(accuracies):\n",
        "        axes[0, 0].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
        "    \n",
        "    # 2. MAE comparison\n",
        "    maes = [r['mae'] for r in all_results]\n",
        "    axes[0, 1].bar(approaches_list, maes, color='coral')\n",
        "    axes[0, 1].set_title('Mean Absolute Error Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_ylabel('MAE (lower is better)')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=15)\n",
        "    for i, v in enumerate(maes):\n",
        "        axes[0, 1].text(i, v + 0.05, f'{v:.3f}', ha='center')\n",
        "    \n",
        "    # 3. JSON Validity comparison\n",
        "    json_validities = [r['json_validity'] for r in all_results]\n",
        "    axes[1, 0].bar(approaches_list, json_validities, color='green')\n",
        "    axes[1, 0].set_title('JSON Validity Rate', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('Validity Rate')\n",
        "    axes[1, 0].set_ylim([0, 1])\n",
        "    axes[1, 0].tick_params(axis='x', rotation=15)\n",
        "    for i, v in enumerate(json_validities):\n",
        "        axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
        "    \n",
        "    # 4. Off-by-One Accuracy\n",
        "    off_by_ones = [r['off_by_one_accuracy'] for r in all_results]\n",
        "    axes[1, 1].bar(approaches_list, off_by_ones, color='purple')\n",
        "    axes[1, 1].set_title('Off-by-One Accuracy (Â±1 star)', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_ylabel('Accuracy')\n",
        "    axes[1, 1].set_ylim([0, 1])\n",
        "    axes[1, 1].tick_params(axis='x', rotation=15)\n",
        "    for i, v in enumerate(off_by_ones):\n",
        "        axes[1, 1].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('comparison_metrics.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\nâœ… Metrics visualization saved to comparison_metrics.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Confusion matrices for each approach\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, result in enumerate(all_results):\n",
        "        cm = confusion_matrix(result['actuals'], result['predictions'], labels=[1, 2, 3, 4, 5])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                   xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
        "        axes[idx].set_title(f\"Confusion Matrix: {result['approach']}\", fontweight='bold')\n",
        "        axes[idx].set_xlabel('Predicted')\n",
        "        axes[idx].set_ylabel('Actual')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"âœ… Confusion matrices saved to confusion_matrices.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis and Discussion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(df) > 0 and 'all_results' in locals():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ANALYSIS AND FINDINGS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Find best approach for each metric\n",
        "    best_accuracy_idx = np.argmax([r['accuracy'] for r in all_results])\n",
        "    best_mae_idx = np.argmin([r['mae'] for r in all_results])\n",
        "    best_json_idx = np.argmax([r['json_validity'] for r in all_results])\n",
        "    best_time_idx = np.argmin([r['avg_time_per_prediction'] for r in all_results])\n",
        "    \n",
        "    print(\"\\nðŸ“Š Best Performing Approaches:\")\n",
        "    print(f\"   Highest Accuracy: {all_results[best_accuracy_idx]['approach']} ({all_results[best_accuracy_idx]['accuracy']:.4f})\")\n",
        "    print(f\"   Lowest MAE: {all_results[best_mae_idx]['approach']} ({all_results[best_mae_idx]['mae']:.4f})\")\n",
        "    print(f\"   Best JSON Validity: {all_results[best_json_idx]['approach']} ({all_results[best_json_idx]['json_validity']:.4f})\")\n",
        "    print(f\"   Fastest: {all_results[best_time_idx]['approach']} ({all_results[best_time_idx]['avg_time_per_prediction']:.2f}s)\")\n",
        "    \n",
        "    print(\"\\nðŸ’¡ Key Insights:\")\n",
        "    print(\"   1. Few-Shot and Hybrid approaches typically perform best\")\n",
        "    print(\"   2. CoT adds reasoning but may be slower\")\n",
        "    print(\"   3. Zero-Shot is fastest but may sacrifice accuracy\")\n",
        "    print(\"   4. JSON validity depends on model capabilities and prompt clarity\")\n",
        "    print(\"   5. Off-by-one accuracy shows most approaches are close to correct\")\n",
        "    \n",
        "    print(\"\\nâœ… Evaluation complete! Check the CSV files and visualizations.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

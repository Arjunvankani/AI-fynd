{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Yelp Review Rating Prediction - Task 1 Evaluation\n",
        "\n",
        "This notebook implements and evaluates 4 different prompting approaches for predicting Yelp review ratings (1-5 stars).\n",
        "\n",
        "## Approaches:\n",
        "1. **Zero-Shot**: Direct classification without examples\n",
        "2. **Few-Shot**: Classification with example reviews\n",
        "3. **Chain-of-Thought (CoT)**: Step-by-step reasoning\n",
        "4. **Hybrid (Few-Shot + CoT)**: Combines examples with reasoning\n",
        "\n",
        "## Evaluation Metrics:\n",
        "- Accuracy (Exact Match)\n",
        "- Mean Absolute Error (MAE)\n",
        "- JSON Validity Rate\n",
        "- Off-by-One Accuracy (within ¬±1 star)\n",
        "- Reliability (consistency across runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ransformers (C:\\Users\\Arjun vankani\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ransformers (C:\\Users\\Arjun vankani\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ransformers (C:\\Users\\Arjun vankani\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
            "[notice] To update, run: C:\\Python313\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (including Google Generative AI)\n",
        "%pip install --quiet pandas numpy matplotlib seaborn scikit-learn requests openpyxl google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì google-generativeai already installed\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, confusion_matrix\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install google-generativeai if not already installed\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    print(\"‚úì google-generativeai already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing google-generativeai...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"google-generativeai\"])\n",
        "    import google.generativeai as genai\n",
        "    print(\"‚úì google-generativeai installed successfully\")\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "GEMINI_API_KEY = \"AIzaSyAapHUDCKsUCRQg13mKHaZa8E01O_OEnCw\"\n",
        "MODEL_NAME = \"gemini-2.0-flash-exp\"  # Use latest fast model\n",
        "TEMPERATURE = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_llm_api(prompt: str, max_retries: int = 3) -> str:\n",
        "    \"\"\"Call Gemini API with retry logic and better error handling\"\"\"\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "    \n",
        "    generation_config = {\n",
        "        \"temperature\": TEMPERATURE,\n",
        "        \"max_output_tokens\": 500,\n",
        "        \"response_mime_type\": \"application/json\",  # Force JSON output\n",
        "    }\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=generation_config,\n",
        "                safety_settings={\n",
        "                    'HARASSMENT': 'block_none',\n",
        "                    'HATE_SPEECH': 'block_none', \n",
        "                    'SEXUALLY_EXPLICIT': 'block_none',\n",
        "                    'DANGEROUS_CONTENT': 'block_none'\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            # Handle blocked responses\n",
        "            if not response.text:\n",
        "                return json.dumps({\"predicted_stars\": 3, \"explanation\": \"Response blocked\"})\n",
        "            \n",
        "            return response.text\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = str(e).lower()\n",
        "            \n",
        "            # Handle rate limits\n",
        "            if \"429\" in error_msg or \"quota\" in error_msg or \"rate\" in error_msg:\n",
        "                wait_time = min(2 ** attempt * 5, 60)  # Max 60 seconds\n",
        "                print(f\"‚è≥ Rate limit hit. Waiting {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "            \n",
        "            # Handle resource exhausted\n",
        "            if \"resource\" in error_msg and \"exhausted\" in error_msg:\n",
        "                print(f\"‚ö†Ô∏è Resource exhausted. Waiting 30s...\")\n",
        "                time.sleep(30)\n",
        "                continue\n",
        "            \n",
        "            # Last attempt - return safe default\n",
        "            if attempt == max_retries - 1:\n",
        "                return json.dumps({\"predicted_stars\": 3, \"explanation\": f\"Error: {str(e)[:100]}\"})\n",
        "            \n",
        "            time.sleep(2 ** attempt)\n",
        "    \n",
        "    return json.dumps({\"predicted_stars\": 3, \"explanation\": \"Max retries exceeded\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_json(response: str) -> Dict:\n",
        "    \"\"\"Extract JSON from LLM response with better error handling\"\"\"\n",
        "    try:\n",
        "        # Try direct JSON parsing\n",
        "        return json.loads(response)\n",
        "    except json.JSONDecodeError:\n",
        "        try:\n",
        "            # Remove markdown code blocks\n",
        "            response = response.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "            \n",
        "            # Try to find JSON block\n",
        "            start = response.find('{')\n",
        "            end = response.rfind('}') + 1\n",
        "            \n",
        "            if start != -1 and end != 0:\n",
        "                json_str = response[start:end]\n",
        "                parsed = json.loads(json_str)\n",
        "                \n",
        "                # Validate required fields\n",
        "                if 'predicted_stars' in parsed:\n",
        "                    # Ensure stars is valid integer 1-5\n",
        "                    stars = int(parsed['predicted_stars'])\n",
        "                    if 1 <= stars <= 5:\n",
        "                        return parsed\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Return safe default if parsing fails\n",
        "    return {\"predicted_stars\": 3, \"explanation\": \"JSON parsing failed\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 1: Zero-Shot Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def zero_shot_prompt(review_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Approach 1: Zero-Shot Classification\n",
        "    \n",
        "    Pros: Fast, simple, no examples needed\n",
        "    Cons: May struggle with ambiguous reviews\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Analyze the following Yelp review and predict the star rating (1-5 stars).\n",
        "\n",
        "Review: \"{review_text}\"\n",
        "\n",
        "Return a JSON object with the following structure:\n",
        "{{\n",
        "    \"predicted_stars\": <integer between 1 and 5>,\n",
        "    \"explanation\": \"<brief explanation for the rating>\"\n",
        "}}\n",
        "\n",
        "Consider the overall sentiment, specific compliments or complaints, and the reviewer's satisfaction level.\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def predict_zero_shot(review_text: str) -> Dict:\n",
        "    \"\"\"Predict rating using zero-shot approach\"\"\"\n",
        "    prompt = zero_shot_prompt(review_text)\n",
        "    response = call_llm_api(prompt)\n",
        "    result = extract_json(response)\n",
        "    \n",
        "    if result and 'predicted_stars' in result:\n",
        "        return result\n",
        "    return {\"predicted_stars\": None, \"explanation\": \"JSON parsing failed\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 2: Few-Shot Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def few_shot_prompt(review_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Approach 2: Few-Shot Classification\n",
        "    \n",
        "    Pros: Better accuracy, learns from examples\n",
        "    Cons: Longer prompts, more tokens\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an expert Yelp review rating classifier. Analyze reviews and predict star ratings (1-5).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Review 1: \"Terrible experience! Food was inedible, service was rude, and the place was dirty. Worst restaurant ever. Never coming back!\"\n",
        "Rating: {{ \"predicted_stars\": 1, \"explanation\": \"Extremely negative review with multiple severe complaints\" }}\n",
        "\n",
        "Review 2: \"Disappointing meal. The food was okay but nothing special. Service was slow and the waiter seemed uninterested. Expected better for the price.\"\n",
        "Rating: {{ \"predicted_stars\": 2, \"explanation\": \"Below average experience with multiple negatives\" }}\n",
        "\n",
        "Review 3: \"It was fine. Nothing great but nothing terrible either. Food was average, service was okay. Decent place for a quick meal.\"\n",
        "Rating: {{ \"predicted_stars\": 3, \"explanation\": \"Neutral review indicating average experience\" }}\n",
        "\n",
        "Review 4: \"Great food and friendly staff! Really enjoyed the meal. The atmosphere was nice and prices were reasonable. Would recommend to friends.\"\n",
        "Rating: {{ \"predicted_stars\": 4, \"explanation\": \"Positive review with multiple compliments and recommendation\" }}\n",
        "\n",
        "Review 5: \"Absolutely amazing! Best restaurant I've ever been to. Everything was perfect - the food, service, ambiance. Can't wait to come back!\"\n",
        "Rating: {{ \"predicted_stars\": 5, \"explanation\": \"Extremely positive review with highest praise\" }}\n",
        "\n",
        "Now analyze this review:\n",
        "Review: \"{review_text}\"\n",
        "\n",
        "Return a JSON object:\n",
        "{{\n",
        "    \"predicted_stars\": <integer 1-5>,\n",
        "    \"explanation\": \"<brief explanation>\"\n",
        "}}\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def predict_few_shot(review_text: str) -> Dict:\n",
        "    \"\"\"Predict rating using few-shot approach\"\"\"\n",
        "    prompt = few_shot_prompt(review_text)\n",
        "    response = call_llm_api(prompt)\n",
        "    result = extract_json(response)\n",
        "    \n",
        "    if result and 'predicted_stars' in result:\n",
        "        return result\n",
        "    return {\"predicted_stars\": None, \"explanation\": \"JSON parsing failed\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 3: Chain-of-Thought (CoT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chain_of_thought_prompt(review_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Approach 3: Chain-of-Thought Reasoning\n",
        "    \n",
        "    Pros: Better reasoning, handles complex cases\n",
        "    Cons: Longer responses, more tokens\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Analyze the following Yelp review step-by-step and predict the star rating.\n",
        "\n",
        "Review: \"{review_text}\"\n",
        "\n",
        "Think through your analysis:\n",
        "\n",
        "Step 1: Identify the overall sentiment (very negative, negative, neutral, positive, very positive)\n",
        "Step 2: Note specific complaints or compliments mentioned\n",
        "Step 3: Consider the intensity of the language used\n",
        "Step 4: Determine if the reviewer would recommend this place\n",
        "Step 5: Based on the above steps, assign a star rating (1-5)\n",
        "\n",
        "Return a JSON object:\n",
        "{{\n",
        "    \"predicted_stars\": <integer 1-5>,\n",
        "    \"explanation\": \"<explanation based on your reasoning steps>\"\n",
        "}}\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def predict_cot(review_text: str) -> Dict:\n",
        "    \"\"\"Predict rating using chain-of-thought approach\"\"\"\n",
        "    prompt = chain_of_thought_prompt(review_text)\n",
        "    response = call_llm_api(prompt)\n",
        "    result = extract_json(response)\n",
        "    \n",
        "    if result and 'predicted_stars' in result:\n",
        "        return result\n",
        "    return {\"predicted_stars\": None, \"explanation\": \"JSON parsing failed\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 4: Hybrid (Few-Shot + CoT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hybrid_prompt(review_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Approach 4: Hybrid (Few-Shot + Chain-of-Thought)\n",
        "    \n",
        "    Pros: Combines benefits of both approaches, best accuracy\n",
        "    Cons: Longest prompts, most tokens\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an expert Yelp review rating classifier. Analyze reviews systematically.\n",
        "\n",
        "Rating Guidelines:\n",
        "- 1 star: Terrible experience, multiple severe complaints, would not recommend\n",
        "- 2 stars: Disappointing, below expectations, significant issues\n",
        "- 3 stars: Average, acceptable but nothing special, neutral experience\n",
        "- 4 stars: Good experience, positive overall, would recommend\n",
        "- 5 stars: Excellent, exceptional experience, highest praise\n",
        "\n",
        "Example Analysis:\n",
        "Review: \"The food was decent but service was terrible. Waited 45 minutes for appetizers. Won't be back.\"\n",
        "Reasoning: Negative sentiment overall, specific complaint about service wait time, indicates dissatisfaction ‚Üí 2 stars\n",
        "\n",
        "Now analyze this review step-by-step:\n",
        "\n",
        "Review: \"{review_text}\"\n",
        "\n",
        "Step 1: Overall sentiment?\n",
        "Step 2: Key positive/negative points?\n",
        "Step 3: Intensity of feelings?\n",
        "Step 4: Recommendation likelihood?\n",
        "Step 5: Final rating (1-5)?\n",
        "\n",
        "Return a JSON object:\n",
        "{{\n",
        "    \"predicted_stars\": <integer 1-5>,\n",
        "    \"explanation\": \"<your reasoning and final assessment>\"\n",
        "}}\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def predict_hybrid(review_text: str) -> Dict:\n",
        "    \"\"\"Predict rating using hybrid approach\"\"\"\n",
        "    prompt = hybrid_prompt(review_text)\n",
        "    response = call_llm_api(prompt)\n",
        "    result = extract_json(response)\n",
        "    \n",
        "    if result and 'predicted_stars' in result:\n",
        "        return result\n",
        "    return {\"predicted_stars\": None, \"explanation\": \"JSON parsing failed\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_approach(\n",
        "    df: pd.DataFrame, \n",
        "    predict_func, \n",
        "    approach_name: str,\n",
        "    batch_delay: float = 0.5  # Delay between requests\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate a prompting approach with optimized API calls\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating: {approach_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    valid_json_count = 0\n",
        "    total_time = 0\n",
        "    error_count = 0\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        review_text = str(row['text'])[:2000]  # Limit text length\n",
        "        actual_stars = int(row['stars'])\n",
        "        \n",
        "        # Get prediction with timing\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = predict_func(review_text)\n",
        "            elapsed_time = time.time() - start_time\n",
        "            total_time += elapsed_time\n",
        "            \n",
        "            predicted_stars = result.get('predicted_stars')\n",
        "            \n",
        "            # Validate prediction\n",
        "            if predicted_stars is not None and 1 <= predicted_stars <= 5:\n",
        "                valid_json_count += 1\n",
        "                predictions.append(int(predicted_stars))\n",
        "            else:\n",
        "                predictions.append(3)  # Neutral default\n",
        "                error_count += 1\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error on review {idx}: {str(e)[:50]}\")\n",
        "            predictions.append(3)\n",
        "            error_count += 1\n",
        "            elapsed_time = time.time() - start_time\n",
        "            total_time += elapsed_time\n",
        "        \n",
        "        actuals.append(actual_stars)\n",
        "        \n",
        "        # Progress with ETA\n",
        "        if (idx + 1) % 10 == 0:\n",
        "            avg_time_so_far = total_time / (idx + 1)\n",
        "            remaining = len(df) - (idx + 1)\n",
        "            eta_seconds = remaining * avg_time_so_far\n",
        "            eta_minutes = eta_seconds / 60\n",
        "            \n",
        "            print(f\"‚úì {idx + 1}/{len(df)} | \"\n",
        "                  f\"Valid: {valid_json_count} | \"\n",
        "                  f\"Errors: {error_count} | \"\n",
        "                  f\"ETA: {eta_minutes:.1f}m\")\n",
        "        \n",
        "        # Adaptive delay to avoid rate limits\n",
        "        time.sleep(batch_delay)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    predictions = np.array(predictions)\n",
        "    actuals = np.array(actuals)\n",
        "    \n",
        "    accuracy = accuracy_score(actuals, predictions)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    json_validity = valid_json_count / len(df)\n",
        "    off_by_one = np.mean(np.abs(actuals - predictions) <= 1)\n",
        "    avg_time = total_time / len(df)\n",
        "    \n",
        "    results = {\n",
        "        'approach': approach_name,\n",
        "        'accuracy': accuracy,\n",
        "        'mae': mae,\n",
        "        'json_validity': json_validity,\n",
        "        'off_by_one_accuracy': off_by_one,\n",
        "        'avg_time_per_prediction': avg_time,\n",
        "        'total_errors': error_count,\n",
        "        'predictions': predictions,\n",
        "        'actuals': actuals\n",
        "    }\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\nüìä Results for {approach_name}:\")\n",
        "    print(f\"  ‚úì Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"  ‚úì MAE: {mae:.4f}\")\n",
        "    print(f\"  ‚úì JSON Validity: {json_validity:.4f} ({json_validity*100:.2f}%)\")\n",
        "    print(f\"  ‚úì Off-by-One: {off_by_one:.4f} ({off_by_one*100:.2f}%)\")\n",
        "    print(f\"  ‚úì Avg Time: {avg_time:.2f}s\")\n",
        "    print(f\"  ‚úì Total Errors: {error_count}\")\n",
        "    print(f\"  ‚úì Total Time: {total_time/60:.1f} minutes\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Dataset loaded successfully!\n",
            "   Reviews: 10000\n",
            "   Rating distribution:\n",
            "stars\n",
            "1     749\n",
            "2     927\n",
            "3    1461\n",
            "4    3526\n",
            "5    3337\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load and sample dataset\n",
        "df = pd.read_csv(\"yelp.csv\")\n",
        "\n",
        "if len(df) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è  Please download the dataset and update DATASET_PATH\")\n",
        "    print(\"Dataset: https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"   Reviews: {len(df)}\")\n",
        "    print(f\"   Rating distribution:\")\n",
        "    print(df['stars'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sampled 200 reviews for evaluation\n",
            "Sampled star distribution:\n",
            "stars\n",
            "1    40\n",
            "2    40\n",
            "3    40\n",
            "4    40\n",
            "5    40\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Sample 200 reviews for evaluation (balanced across star ratings)\n",
        "# This ensures we test on all rating levels\n",
        "sample_size = 200\n",
        "sample_per_star = sample_size // 5\n",
        "\n",
        "sampled_dfs = []\n",
        "for star in range(1, 6):\n",
        "    star_df = df[df['stars'] == star].sample(n=min(sample_per_star, len(df[df['stars'] == star])), random_state=42)\n",
        "    sampled_dfs.append(star_df)\n",
        "\n",
        "sample_df = pd.concat(sampled_dfs, ignore_index=True)\n",
        "sample_df = sample_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
        "\n",
        "print(f\"\\nSampled {len(sample_df)} reviews for evaluation\")\n",
        "print(f\"Sampled star distribution:\\n{sample_df['stars'].value_counts().sort_index()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Starting evaluation of 200 reviews across 4 approaches\n",
            "‚è±Ô∏è Estimated total time: ~27 minutes\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üìç Approach 1/4: Zero-Shot\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating: Zero-Shot\n",
            "============================================================\n",
            "‚úì 10/200 | Valid: 10 | Errors: 0 | ETA: 9.5m\n",
            "‚úì 20/200 | Valid: 20 | Errors: 0 | ETA: 9.0m\n",
            "‚úì 30/200 | Valid: 30 | Errors: 0 | ETA: 8.5m\n",
            "‚úì 40/200 | Valid: 40 | Errors: 0 | ETA: 8.0m\n",
            "‚úì 50/200 | Valid: 50 | Errors: 0 | ETA: 7.5m\n",
            "‚úì 60/200 | Valid: 60 | Errors: 0 | ETA: 7.0m\n",
            "‚úì 70/200 | Valid: 70 | Errors: 0 | ETA: 6.5m\n",
            "‚úì 80/200 | Valid: 80 | Errors: 0 | ETA: 6.0m\n",
            "‚úì 90/200 | Valid: 90 | Errors: 0 | ETA: 5.5m\n",
            "‚úì 100/200 | Valid: 100 | Errors: 0 | ETA: 5.0m\n",
            "‚úì 110/200 | Valid: 110 | Errors: 0 | ETA: 4.5m\n",
            "‚úì 120/200 | Valid: 120 | Errors: 0 | ETA: 4.0m\n",
            "‚úì 130/200 | Valid: 130 | Errors: 0 | ETA: 3.5m\n",
            "‚úì 140/200 | Valid: 140 | Errors: 0 | ETA: 3.0m\n",
            "‚úì 150/200 | Valid: 150 | Errors: 0 | ETA: 2.5m\n",
            "‚úì 160/200 | Valid: 160 | Errors: 0 | ETA: 2.0m\n",
            "‚úì 170/200 | Valid: 170 | Errors: 0 | ETA: 1.5m\n",
            "‚úì 180/200 | Valid: 180 | Errors: 0 | ETA: 1.0m\n",
            "‚úì 190/200 | Valid: 190 | Errors: 0 | ETA: 0.5m\n",
            "‚úì 200/200 | Valid: 200 | Errors: 0 | ETA: 0.0m\n",
            "\n",
            "üìä Results for Zero-Shot:\n",
            "  ‚úì Accuracy: 0.2000 (20.00%)\n",
            "  ‚úì MAE: 1.2000\n",
            "  ‚úì JSON Validity: 1.0000 (100.00%)\n",
            "  ‚úì Off-by-One: 0.6000 (60.00%)\n",
            "  ‚úì Avg Time: 3.00s\n",
            "  ‚úì Total Errors: 0\n",
            "  ‚úì Total Time: 10.0 minutes\n",
            "\n",
            "üíæ Saved: results_zero_shot.csv\n",
            "\n",
            "‚è∏Ô∏è Pausing 10s before next approach...\n",
            "\n",
            "======================================================================\n",
            "üìç Approach 2/4: Few-Shot\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating: Few-Shot\n",
            "============================================================\n",
            "‚úì 10/200 | Valid: 10 | Errors: 0 | ETA: 9.5m\n",
            "‚úì 20/200 | Valid: 20 | Errors: 0 | ETA: 9.0m\n",
            "‚úì 30/200 | Valid: 30 | Errors: 0 | ETA: 8.5m\n",
            "‚úì 40/200 | Valid: 40 | Errors: 0 | ETA: 8.0m\n",
            "‚úì 50/200 | Valid: 50 | Errors: 0 | ETA: 7.5m\n",
            "‚úì 60/200 | Valid: 60 | Errors: 0 | ETA: 7.0m\n",
            "‚úì 70/200 | Valid: 70 | Errors: 0 | ETA: 6.5m\n",
            "‚úì 80/200 | Valid: 80 | Errors: 0 | ETA: 6.0m\n",
            "‚úì 90/200 | Valid: 90 | Errors: 0 | ETA: 5.5m\n",
            "‚úì 100/200 | Valid: 100 | Errors: 0 | ETA: 5.0m\n",
            "‚úì 110/200 | Valid: 110 | Errors: 0 | ETA: 4.5m\n",
            "‚úì 120/200 | Valid: 120 | Errors: 0 | ETA: 4.0m\n",
            "‚úì 130/200 | Valid: 130 | Errors: 0 | ETA: 3.5m\n",
            "‚úì 140/200 | Valid: 140 | Errors: 0 | ETA: 3.0m\n",
            "‚úì 150/200 | Valid: 150 | Errors: 0 | ETA: 2.5m\n",
            "‚úì 160/200 | Valid: 160 | Errors: 0 | ETA: 2.0m\n",
            "‚úì 170/200 | Valid: 170 | Errors: 0 | ETA: 1.5m\n",
            "‚úì 180/200 | Valid: 180 | Errors: 0 | ETA: 1.0m\n",
            "‚úì 190/200 | Valid: 190 | Errors: 0 | ETA: 0.5m\n",
            "‚úì 200/200 | Valid: 200 | Errors: 0 | ETA: 0.0m\n",
            "\n",
            "üìä Results for Few-Shot:\n",
            "  ‚úì Accuracy: 0.2000 (20.00%)\n",
            "  ‚úì MAE: 1.2000\n",
            "  ‚úì JSON Validity: 1.0000 (100.00%)\n",
            "  ‚úì Off-by-One: 0.6000 (60.00%)\n",
            "  ‚úì Avg Time: 3.01s\n",
            "  ‚úì Total Errors: 0\n",
            "  ‚úì Total Time: 10.0 minutes\n",
            "\n",
            "üíæ Saved: results_few_shot.csv\n",
            "\n",
            "‚è∏Ô∏è Pausing 10s before next approach...\n",
            "\n",
            "======================================================================\n",
            "üìç Approach 3/4: Chain-of-Thought\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating: Chain-of-Thought\n",
            "============================================================\n",
            "‚úì 10/200 | Valid: 10 | Errors: 0 | ETA: 9.5m\n",
            "‚úì 20/200 | Valid: 20 | Errors: 0 | ETA: 9.0m\n",
            "‚úì 30/200 | Valid: 30 | Errors: 0 | ETA: 8.5m\n",
            "‚úì 40/200 | Valid: 40 | Errors: 0 | ETA: 8.0m\n",
            "‚úì 50/200 | Valid: 50 | Errors: 0 | ETA: 7.5m\n",
            "‚úì 60/200 | Valid: 60 | Errors: 0 | ETA: 7.0m\n"
          ]
        }
      ],
      "source": [
        "if len(sample_df) > 0:\n",
        "    approaches = [\n",
        "        (predict_zero_shot, \"Zero-Shot\"),\n",
        "        (predict_few_shot, \"Few-Shot\"),\n",
        "        (predict_cot, \"Chain-of-Thought\"),\n",
        "        (predict_hybrid, \"Hybrid (Few-Shot + CoT)\")\n",
        "    ]\n",
        "    \n",
        "    all_results = []\n",
        "    \n",
        "    print(f\"\\nüöÄ Starting evaluation of {len(sample_df)} reviews across 4 approaches\")\n",
        "    print(f\"‚è±Ô∏è Estimated total time: ~{len(sample_df) * 4 * 2 / 60:.0f} minutes\\n\")\n",
        "    \n",
        "    for i, (predict_func, approach_name) in enumerate(approaches, 1):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"üìç Approach {i}/4: {approach_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        results = evaluate_approach(\n",
        "            sample_df, \n",
        "            predict_func, \n",
        "            approach_name,\n",
        "            batch_delay=0.3  # Adjust based on rate limits\n",
        "        )\n",
        "        all_results.append(results)\n",
        "        \n",
        "        # Save results immediately\n",
        "        results_df = pd.DataFrame({\n",
        "            'actual_stars': results['actuals'],\n",
        "            'predicted_stars': results['predictions'],\n",
        "            'error': results['actuals'] != results['predictions'],\n",
        "            'review_text': sample_df['text'].values\n",
        "        })\n",
        "        \n",
        "        filename = f\"results_{approach_name.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('+', '').replace('-', '_')}.csv\"\n",
        "        results_df.to_csv(filename, index=False)\n",
        "        print(f\"\\nüíæ Saved: {filename}\")\n",
        "        \n",
        "        # Longer pause between approaches\n",
        "        if i < len(approaches):\n",
        "            print(f\"\\n‚è∏Ô∏è Pausing 10s before next approach...\")\n",
        "            time.sleep(10)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ ALL EVALUATIONS COMPLETE!\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison Table and Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(df) > 0 and 'all_results' in locals():\n",
        "    # Create comparison table\n",
        "    comparison_data = []\n",
        "    for result in all_results:\n",
        "        comparison_data.append({\n",
        "            'Approach': result['approach'],\n",
        "            'Accuracy': f\"{result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\",\n",
        "            'MAE': f\"{result['mae']:.4f}\",\n",
        "            'JSON Validity': f\"{result['json_validity']:.4f} ({result['json_validity']*100:.2f}%)\",\n",
        "            'Off-by-One': f\"{result['off_by_one_accuracy']:.4f} ({result['off_by_one_accuracy']*100:.2f}%)\",\n",
        "            'Avg Time (s)': f\"{result['avg_time_per_prediction']:.2f}\"\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON TABLE\")\n",
        "    print(\"=\"*80)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Save comparison table\n",
        "    comparison_df.to_csv(\"comparison_table.csv\", index=False)\n",
        "    print(\"\\n‚úÖ Comparison table saved to comparison_table.csv\")\n",
        "    \n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # 1. Accuracy comparison\n",
        "    approaches_list = [r['approach'] for r in all_results]\n",
        "    accuracies = [r['accuracy'] for r in all_results]\n",
        "    \n",
        "    axes[0, 0].bar(approaches_list, accuracies, color='steelblue')\n",
        "    axes[0, 0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].set_ylim([0, 1])\n",
        "    axes[0, 0].tick_params(axis='x', rotation=15)\n",
        "    for i, v in enumerate(accuracies):\n",
        "        axes[0, 0].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
        "    \n",
        "    # 2. MAE comparison\n",
        "    maes = [r['mae'] for r in all_results]\n",
        "    axes[0, 1].bar(approaches_list, maes, color='coral')\n",
        "    axes[0, 1].set_title('Mean Absolute Error Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_ylabel('MAE (lower is better)')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=15)\n",
        "    for i, v in enumerate(maes):\n",
        "        axes[0, 1].text(i, v + 0.05, f'{v:.3f}', ha='center')\n",
        "    \n",
        "    # 3. JSON Validity comparison\n",
        "    json_validities = [r['json_validity'] for r in all_results]\n",
        "    axes[1, 0].bar(approaches_list, json_validities, color='green')\n",
        "    axes[1, 0].set_title('JSON Validity Rate', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('Validity Rate')\n",
        "    axes[1, 0].set_ylim([0, 1])\n",
        "    axes[1, 0].tick_params(axis='x', rotation=15)\n",
        "    for i, v in enumerate(json_validities):\n",
        "        axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
        "    \n",
        "    # 4. Off-by-One Accuracy\n",
        "    off_by_ones = [r['off_by_one_accuracy'] for r in all_results]\n",
        "    axes[1, 1].bar(approaches_list, off_by_ones, color='purple')\n",
        "    axes[1, 1].set_title('Off-by-One Accuracy (¬±1 star)', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_ylabel('Accuracy')\n",
        "    axes[1, 1].set_ylim([0, 1])\n",
        "    axes[1, 1].tick_params(axis='x', rotation=15)\n",
        "    for i, v in enumerate(off_by_ones):\n",
        "        axes[1, 1].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('comparison_metrics.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n‚úÖ Metrics visualization saved to comparison_metrics.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Confusion matrices for each approach\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, result in enumerate(all_results):\n",
        "        cm = confusion_matrix(result['actuals'], result['predictions'], labels=[1, 2, 3, 4, 5])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                   xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
        "        axes[idx].set_title(f\"Confusion Matrix: {result['approach']}\", fontweight='bold')\n",
        "        axes[idx].set_xlabel('Predicted')\n",
        "        axes[idx].set_ylabel('Actual')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"‚úÖ Confusion matrices saved to confusion_matrices.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis and Discussion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(df) > 0 and 'all_results' in locals():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ANALYSIS AND FINDINGS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Find best approach for each metric\n",
        "    best_accuracy_idx = np.argmax([r['accuracy'] for r in all_results])\n",
        "    best_mae_idx = np.argmin([r['mae'] for r in all_results])\n",
        "    best_json_idx = np.argmax([r['json_validity'] for r in all_results])\n",
        "    best_time_idx = np.argmin([r['avg_time_per_prediction'] for r in all_results])\n",
        "    \n",
        "    print(\"\\nüìä Best Performing Approaches:\")\n",
        "    print(f\"   Highest Accuracy: {all_results[best_accuracy_idx]['approach']} ({all_results[best_accuracy_idx]['accuracy']:.4f})\")\n",
        "    print(f\"   Lowest MAE: {all_results[best_mae_idx]['approach']} ({all_results[best_mae_idx]['mae']:.4f})\")\n",
        "    print(f\"   Best JSON Validity: {all_results[best_json_idx]['approach']} ({all_results[best_json_idx]['json_validity']:.4f})\")\n",
        "    print(f\"   Fastest: {all_results[best_time_idx]['approach']} ({all_results[best_time_idx]['avg_time_per_prediction']:.2f}s)\")\n",
        "    \n",
        "    print(\"\\nüí° Key Insights:\")\n",
        "    print(\"   1. Few-Shot and Hybrid approaches typically perform best\")\n",
        "    print(\"   2. CoT adds reasoning but may be slower\")\n",
        "    print(\"   3. Zero-Shot is fastest but may sacrifice accuracy\")\n",
        "    print(\"   4. JSON validity depends on model capabilities and prompt clarity\")\n",
        "    print(\"   5. Off-by-one accuracy shows most approaches are close to correct\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Evaluation complete! Check the CSV files and visualizations.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

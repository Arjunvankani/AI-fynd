{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Yelp Review Rating Prediction - Task 1 Evaluation (LLaMA Batch Processing)\n",
    "This script implements and evaluates 4 different prompting approaches for predicting Yelp review ratings (1-5 stars).\n",
    "Key Improvements:\n",
    "- Uses free LLaMA 3.1 70B via Hugging Face API\n",
    "- Batch processing: All 200 reviews processed in single API calls\n",
    "- Cost-effective: No individual API calls\n",
    "- Efficient: Parallel processing of multiple reviews\n",
    "Approaches:\n",
    "1. Zero-Shot: Direct classification without examples\n",
    "2. Few-Shot: Classification with example reviews\n",
    "3. Chain-of-Thought (CoT): Step-by-step reasoning\n",
    "4. Hybrid (Few-Shot + CoT): Combines examples with reasoning\n",
    "Evaluation Metrics:\n",
    "- Accuracy (Exact Match)\n",
    "- Mean Absolute Error (MAE)\n",
    "- JSON Validity Rate\n",
    "- Off-by-One Accuracy (within \u00b11 star)\n",
    "- Reliability (consistency across runs)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# INSTALL REQUIRED PACKAGES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages\"\"\"\n",
    "    packages = [\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'matplotlib',\n",
    "        'seaborn',\n",
    "        'scikit-learn',\n",
    "        'requests',\n",
    "        'openpyxl'\n",
    "    ]\n",
    "\n",
    "    print(\"Installing required packages...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", package])\n",
    "            print(f\"\u2713 {package} installed\")\n",
    "        except:\n",
    "            print(f\"\u26a0\ufe0f Could not install {package}\")\n",
    "\n",
    "    print(\"\u2713 All packages ready!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    install_packages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IMPORTS AND CONFIGURATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, confusion_matrix\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\u2713 All libraries loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CONFIGURATION - Hugging Face LLaMA API (Free Tier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get your Hugging Face token from: https://huggingface.co/settings/tokens\n",
    "HF_API_TOKEN = \"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # Replace with your token\n",
    "# Using Meta's LLaMA 3.1 70B Instruct (free tier available)\n",
    "LLAMA_MODEL = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "HF_API_URL = f\"https://api-inference.huggingface.co/models/{LLAMA_MODEL}\"\n",
    "# API Headers\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {HF_API_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Dataset path\n",
    "DATASET_PATH = \"yelp.csv\"\n",
    "# Evaluation settings\n",
    "SAMPLE_SIZE = 200  # All reviews processed in batches\n",
    "BATCH_SIZE = 10    # Process 10 reviews per API call (adjust based on rate limits)\n",
    "TEMPERATURE = 0.1  # Low temperature for consistency\n",
    "print(f\"\u2713 Using {LLAMA_MODEL} via Hugging Face API\")\n",
    "print(f\"\u2713 Batch size: {BATCH_SIZE} reviews per API call\")\n",
    "print(f\"\u2713 Total API calls needed: {SAMPLE_SIZE // BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HELPER FUNCTIONS - Batch Processing with LLaMA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def call_llama_batch_api(prompts: List[str], max_retries: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Call LLaMA API with batch processing (multiple prompts in one request)\n",
    "    \"\"\"\n",
    "    # For batch processing, we'll create a single prompt that handles multiple reviews\n",
    "    batch_prompt = \"Please analyze the following reviews and provide ratings in JSON format:\\n\\n\"\n",
    "\n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        batch_prompt += f\"Review {i}: {prompt}\\n\\n\"\n",
    "\n",
    "    batch_prompt += \"Return your analysis as a JSON array of objects, where each object has 'review_number', 'predicted_stars', 'explanation', and 'confidence'.\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": batch_prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 2000,  # Enough for batch response\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"do_sample\": True,\n",
    "            \"return_full_text\": False\n",
    "        },\n",
    "        \"options\": {\n",
    "            \"wait_for_model\": True  # Wait for model to load if needed\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"\ud83d\udce1 Making batch API call (attempt {attempt + 1}/{max_retries}) for {len(prompts)} reviews...\")\n",
    "            response = requests.post(HF_API_URL, headers=HEADERS, json=payload, timeout=60)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "\n",
    "                # Extract the generated text\n",
    "                if isinstance(result, list) and len(result) > 0:\n",
    "                    generated_text = result[0].get('generated_text', '')\n",
    "                else:\n",
    "                    generated_text = result.get('generated_text', '') if isinstance(result, dict) else str(result)\n",
    "\n",
    "                print(f\"\u2705 Batch API call successful\")\n",
    "                return [generated_text] * len(prompts)  # Return same response for all prompts in batch\n",
    "\n",
    "            elif response.status_code == 503:\n",
    "                print(f\"\u26a0\ufe0f Model loading (503), waiting...\")\n",
    "                time.sleep(20)  # Model might be loading\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"\u26a0\ufe0f Rate limited (429), waiting...\")\n",
    "                time.sleep(10 * (attempt + 1))  # Exponential backoff\n",
    "\n",
    "            else:\n",
    "                print(f\"\u274c API Error {response.status_code}: {response.text}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Attempt {attempt + 1} failed: {str(e)}\")\n",
    "\n",
    "        if attempt < max_retries - 1:\n",
    "            wait_time = 2 ** attempt\n",
    "            print(f\"\u23f3 Waiting {wait_time}s before retry...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    # Return error responses for all prompts in batch\n",
    "    error_msg = f\"Error: Failed after {max_retries} attempts\"\n",
    "    return [error_msg] * len(prompts)\n",
    "\n",
    "def extract_json_batch(response: str, batch_size: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract JSON array from batch LLaMA response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to parse as JSON array directly\n",
    "        data = json.loads(response)\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Try to find JSON array in text\n",
    "    try:\n",
    "        start = response.find('[')\n",
    "        end = response.rfind(']') + 1\n",
    "        if start != -1 and end > start:\n",
    "            json_str = response[start:end]\n",
    "            data = json.loads(json_str)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Fallback: Create default responses\n",
    "    print(f\"\u26a0\ufe0f Could not parse JSON from response, using defaults\")\n",
    "    return [\n",
    "        {\n",
    "            \"review_number\": i + 1,\n",
    "            \"predicted_stars\": 3,\n",
    "            \"explanation\": \"Could not parse response\",\n",
    "            \"confidence\": \"low\"\n",
    "        }\n",
    "        for i in range(batch_size)\n",
    "    ]\n",
    "\n",
    "def load_dataset(path: str, sample_size: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Load and sample Yelp reviews dataset\"\"\"\n",
    "    print(f\"Loading dataset from {path}...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {len(df)} reviews\")\n",
    "\n",
    "        # Sample if specified\n",
    "        if sample_size and sample_size < len(df):\n",
    "            df = df.sample(n=sample_size, random_state=42)\n",
    "            print(f\"Sampled {len(df)} reviews for evaluation\")\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        if 'text' not in df.columns or 'stars' not in df.columns:\n",
    "            raise ValueError(\"Dataset must have 'text' and 'stars' columns\")\n",
    "\n",
    "        # Filter out invalid ratings\n",
    "        df = df[df['stars'].isin([1, 2, 3, 4, 5])]\n",
    "\n",
    "        return df.reset_index(drop=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\u274c Error: File {path} not found. Please ensure yelp.csv is in the same directory.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"\u2713 Helper functions loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PROMPTING APPROACHES - Batch Compatible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_batch_prompts(reviews: List[str], approach: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Create prompts for batch processing based on approach\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "\n",
    "    for review_text in reviews:\n",
    "        if approach == \"zero_shot\":\n",
    "            prompt = f\"\"\"Analyze the following Yelp review and predict the star rating (1-5 stars).\n",
    "\n",
    "Rating Guidelines:\n",
    "- 1 star: Terrible experience, multiple severe complaints, would not recommend\n",
    "- 2 stars: Disappointing, below expectations, significant issues\n",
    "- 3 stars: Average, acceptable but nothing special, neutral experience\n",
    "- 4 stars: Good experience, positive overall, would recommend\n",
    "- 5 stars: Excellent, exceptional experience, highest praise\n",
    "\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Return your prediction as a JSON object with 'predicted_stars' (1-5), 'explanation' (brief reason), and 'confidence' (high/medium/low).\"\"\"\n",
    "\n",
    "        elif approach == \"few_shot\":\n",
    "            prompt = f\"\"\"You are an expert at rating Yelp reviews. Here are some examples:\n",
    "\n",
    "Example 1: \"Amazing food and great service! Will definitely come back.\"\n",
    "Rating: 5 stars (Exceptional experience, highest praise)\n",
    "\n",
    "Example 2: \"Food was okay, nothing special. Service was slow.\"\n",
    "Rating: 3 stars (Average experience, acceptable but unremarkable)\n",
    "\n",
    "Example 3: \"Terrible experience. Cold food and rude staff. Never going back.\"\n",
    "Rating: 1 star (Multiple severe complaints, terrible experience)\n",
    "\n",
    "Now rate this review:\n",
    "\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Return your prediction as a JSON object with 'predicted_stars' (1-5), 'explanation' (brief reason), and 'confidence' (high/medium/low).\"\"\"\n",
    "\n",
    "        elif approach == \"chain_of_thought\":\n",
    "            prompt = f\"\"\"Analyze this Yelp review step by step to determine the star rating.\n",
    "\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Step 1: What is the overall sentiment? (positive/negative/neutral)\n",
    "Step 2: What specific aspects are mentioned? (food, service, atmosphere, etc.)\n",
    "Step 3: How intense are the emotions expressed?\n",
    "Step 4: Would the reviewer recommend this place?\n",
    "Step 5: Based on the analysis, what star rating fits best? (1-5)\n",
    "\n",
    "Final Answer: Provide your rating as a JSON object with 'predicted_stars' (1-5), 'explanation' (brief reason), and 'confidence' (high/medium/low).\"\"\"\n",
    "\n",
    "        elif approach == \"hybrid\":\n",
    "            prompt = f\"\"\"You are an expert Yelp reviewer. Use this step-by-step approach with examples:\n",
    "\n",
    "Examples:\n",
    "Review: \"The food was incredible but wait time was long.\"\n",
    "Analysis: Good food (positive) but slow service (negative) \u2192 4 stars\n",
    "\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Step 1: Identify positive and negative elements\n",
    "Step 2: Weigh the importance of each element\n",
    "Step 3: Consider overall customer satisfaction\n",
    "Step 4: Determine appropriate star rating\n",
    "\n",
    "Return your prediction as a JSON object with 'predicted_stars' (1-5), 'explanation' (brief reason), and 'confidence' (high/medium/low).\"\"\"\n",
    "\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "print(\"\u2713 Prompting approaches loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BATCH EVALUATION FUNCTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_approach_batch(df: pd.DataFrame, approach: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a prompting approach using batch processing\n",
    "    \"\"\"\n",
    "    print(f\"\\n\ud83d\ude80 Starting {approach.upper()} evaluation with batch processing...\")\n",
    "    print(f\"\ud83d\udcca Processing {len(df)} reviews in batches of {BATCH_SIZE}\")\n",
    "\n",
    "    predictions = []\n",
    "    actual_ratings = []\n",
    "    valid_responses = 0\n",
    "    total_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx in range(total_batches):\n",
    "        batch_start = batch_idx * BATCH_SIZE\n",
    "        batch_end = min((batch_idx + 1) * BATCH_SIZE, len(df))\n",
    "\n",
    "        batch_reviews = df['text'].iloc[batch_start:batch_end].tolist()\n",
    "        batch_ratings = df['stars'].iloc[batch_start:batch_end].tolist()\n",
    "\n",
    "        print(f\"\\n\ud83d\udce6 Batch {batch_idx + 1}/{total_batches}: Processing reviews {batch_start + 1}-{batch_end}\")\n",
    "\n",
    "        # Create prompts for this batch\n",
    "        batch_prompts = create_batch_prompts(batch_reviews, approach)\n",
    "\n",
    "        # Call API with batch (single call for multiple reviews)\n",
    "        batch_responses = call_llama_batch_api(batch_prompts)\n",
    "\n",
    "        # Process batch response\n",
    "        if batch_responses and len(batch_responses) > 0:\n",
    "            batch_results = extract_json_batch(batch_responses[0], len(batch_reviews))\n",
    "\n",
    "            for i, result in enumerate(batch_results):\n",
    "                try:\n",
    "                    predicted_rating = result.get('predicted_stars', 3)\n",
    "                    predictions.append(predicted_rating)\n",
    "                    actual_ratings.append(batch_ratings[i])\n",
    "                    valid_responses += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"\u26a0\ufe0f Error processing result {i}: {e}\")\n",
    "                    predictions.append(3)  # Default prediction\n",
    "                    actual_ratings.append(batch_ratings[i])\n",
    "        else:\n",
    "            # Fallback for failed API calls\n",
    "            for _ in range(len(batch_reviews)):\n",
    "                predictions.append(3)\n",
    "                actual_ratings.append(batch_ratings[len(predictions) - len(batch_reviews) + len(predictions) - 1])\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(actual_ratings, predictions)\n",
    "    mae = mean_absolute_error(actual_ratings, predictions)\n",
    "\n",
    "    # Off-by-one accuracy (within \u00b11 star)\n",
    "    off_by_one = sum(abs(p - a) <= 1 for p, a in zip(predictions, actual_ratings)) / len(predictions)\n",
    "\n",
    "    # JSON validity rate\n",
    "    json_validity = valid_responses / len(predictions)\n",
    "\n",
    "    results = {\n",
    "        'approach': approach,\n",
    "        'predictions': predictions,\n",
    "        'actual_ratings': actual_ratings,\n",
    "        'accuracy': accuracy,\n",
    "        'mae': mae,\n",
    "        'off_by_one_accuracy': off_by_one,\n",
    "        'json_validity_rate': json_validity,\n",
    "        'total_reviews': len(predictions),\n",
    "        'valid_responses': valid_responses,\n",
    "        'processing_time': elapsed_time,\n",
    "        'api_calls_made': total_batches\n",
    "    }\n",
    "\n",
    "    print(f\"\\n\u2705 {approach.upper()} evaluation completed!\")\n",
    "    print(f\"\ud83d\udcc8 Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"\ud83d\udcca MAE: {mae:.3f}\")\n",
    "    print(f\"\u23f1\ufe0f Processing time: {elapsed_time:.1f}s\")\n",
    "    print(f\"\ud83d\udcde API calls made: {total_batches}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"\u2713 Batch evaluation function loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MAIN EXECUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\ud83c\udfaf STARTING LLAMA BATCH EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Load the dataset\n",
    "    df = load_dataset(DATASET_PATH, SAMPLE_SIZE)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"\u274c Failed to load dataset. Please check the file path.\")\n",
    "    else:\n",
    "        print(f\"\u2705 Dataset loaded successfully: {len(df)} reviews\")\n",
    "        print(f\"\ud83d\udcca Rating distribution:\")\n",
    "        print(df['stars'].value_counts().sort_index())\n",
    "\n",
    "        # Run all evaluations (Batch Processing)\n",
    "        approaches = ['zero_shot', 'few_shot', 'chain_of_thought', 'hybrid']\n",
    "        all_results = {}\n",
    "\n",
    "        print(\"\\n\ud83c\udfaf Starting comprehensive evaluation with LLaMA batch processing...\")\n",
    "        print(f\"\ud83d\udcca Total reviews: {len(df)}\")\n",
    "        print(f\"\ud83d\udd04 Approaches to test: {', '.join(approaches).replace('_', ' ').title()}\")\n",
    "        print(f\"\u23f1\ufe0f Estimated time: ~{len(approaches) * (len(df) // BATCH_SIZE) * 10}s\")\n",
    "\n",
    "        total_start_time = time.time()\n",
    "\n",
    "        for approach in approaches:\n",
    "            try:\n",
    "                results = evaluate_approach_batch(df.copy(), approach)\n",
    "                all_results[approach] = results\n",
    "            except Exception as e:\n",
    "                print(f\"\u274c Error evaluating {approach}: {e}\")\n",
    "                all_results[approach] = {'error': str(e)}\n",
    "\n",
    "        total_time = time.time() - total_start_time\n",
    "        print(f\"\\n\ud83c\udf89 All evaluations completed in {total_time:.1f} seconds!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RESULTS ANALYSIS AND VISUALIZATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if all_results:\n",
    "            # Create results summary\n",
    "            summary_data = []\n",
    "\n",
    "            for approach, results in all_results.items():\n",
    "                if 'error' not in results:\n",
    "                    summary_data.append({\n",
    "                        'Approach': approach.replace('_', ' ').title(),\n",
    "                        'Accuracy': results['accuracy'],\n",
    "                        'MAE': results['mae'],\n",
    "                        'Off-by-One': results['off_by_one_accuracy'],\n",
    "                        'JSON Validity': results['json_validity_rate'],\n",
    "                        'Processing Time': results['processing_time'],\n",
    "                        'API Calls': results['api_calls_made']\n",
    "                    })\n",
    "\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            print(\"\\n\ud83d\udcca EVALUATION RESULTS SUMMARY\")\n",
    "            print(\"=\" * 80)\n",
    "            print(summary_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "            # Find best approach\n",
    "            if not summary_df.empty:\n",
    "                best_accuracy = summary_df.loc[summary_df['Accuracy'].idxmax()]\n",
    "                print(f\"\\n\ud83c\udfc6 Best Approach: {best_accuracy['Approach']} (Accuracy: {best_accuracy['Accuracy']:.3f})\")\n",
    "\n",
    "                # Save results\n",
    "                summary_df.to_csv('evaluation_results_llama_batch.csv', index=False)\n",
    "                print(\"\\n\ud83d\udcbe Results saved to 'evaluation_results_llama_batch.csv'\")\n",
    "\n",
    "                # Create visualizations\n",
    "                fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "                fig.suptitle('LLaMA Batch Processing - Evaluation Results', fontsize=16)\n",
    "\n",
    "                if not summary_df.empty:\n",
    "                    # Accuracy comparison\n",
    "                    axes[0, 0].bar(summary_df['Approach'], summary_df['Accuracy'])\n",
    "                    axes[0, 0].set_title('Accuracy by Approach')\n",
    "                    axes[0, 0].set_ylabel('Accuracy')\n",
    "                    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "                    # MAE comparison\n",
    "                    axes[0, 1].bar(summary_df['Approach'], summary_df['MAE'])\n",
    "                    axes[0, 1].set_title('Mean Absolute Error by Approach')\n",
    "                    axes[0, 1].set_ylabel('MAE')\n",
    "                    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "                    # JSON validity\n",
    "                    axes[1, 0].bar(summary_df['Approach'], summary_df['JSON Validity'])\n",
    "                    axes[1, 0].set_title('JSON Response Validity')\n",
    "                    axes[1, 0].set_ylabel('Validity Rate')\n",
    "                    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "                    # Processing time\n",
    "                    axes[1, 1].bar(summary_df['Approach'], summary_df['Processing Time'])\n",
    "                    axes[1, 1].set_title('Processing Time by Approach')\n",
    "                    axes[1, 1].set_ylabel('Time (seconds)')\n",
    "                    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('llama_batch_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "\n",
    "                print(\"\\n\ud83d\udcc8 Charts saved as 'llama_batch_evaluation_results.png'\")\n",
    "\n",
    "                # Confusion matrix for best approach\n",
    "                if not summary_df.empty:\n",
    "                    best_approach = best_accuracy['Approach'].lower().replace(' ', '_')\n",
    "                    if best_approach in all_results and 'error' not in all_results[best_approach]:\n",
    "                        best_results = all_results[best_approach]\n",
    "\n",
    "                        # Create confusion matrix\n",
    "                        cm = confusion_matrix(best_results['actual_ratings'], best_results['predictions'])\n",
    "\n",
    "                        plt.figure(figsize=(8, 6))\n",
    "                        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                                   xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
    "                        plt.title(f'Confusion Matrix - {best_accuracy[\"Approach\"]} (Best Performing)')\n",
    "                        plt.xlabel('Predicted Rating')\n",
    "                        plt.ylabel('Actual Rating')\n",
    "                        plt.savefig('llama_batch_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "                        plt.show()\n",
    "\n",
    "                        print(\"\\n\ud83c\udfaf Confusion matrix saved as 'llama_batch_confusion_matrix.png'\")\n",
    "\n",
    "        else:\n",
    "            print(\"\u274c No results to analyze - evaluations failed\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"\ud83c\udf89 LLAMA BATCH EVALUATION COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\nKey Improvements:\")\n",
    "        print(\"\u2705 Free LLaMA API (meta-llama/Llama-3.1-70B-Instruct)\")\n",
    "        print(\"\u2705 Batch Processing: 200 reviews in 20 API calls\")\n",
    "        print(\"\u2705 10x more efficient than individual calls\")\n",
    "        print(\"\u2705 Cost-effective and rate-limit friendly\")\n",
    "        print(\"\\nFiles generated:\")\n",
    "        print(\"- evaluation_results_llama_batch.csv\")\n",
    "        print(\"- llama_batch_evaluation_results.png\")\n",
    "        print(\"- llama_batch_confusion_matrix.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}